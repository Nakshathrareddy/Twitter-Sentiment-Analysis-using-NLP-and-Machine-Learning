# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zBrdzuX-R_B8zXXfPZ8PfowGTV_RfFpM
"""

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize

from nltk.corpus import stopwords

# Downloading required NLTK datasets
nltk.download('punkt')  # Tokenizer models for sentence and word tokenization
nltk.download('punkt_tab')  # Optional: Extra support for tokenization
nltk.download('stopwords')

import pandas as pd

# Loading the CSV dataset into a DataFrame
df = pd.read_csv("https://raw.githubusercontent.com/suhasmaddali/Twitter-Sentiment-Analysis/refs/heads/main/train.csv")

df

import re

cleaned = []  # List to store cleaned text
for text in df['selected_text']:
    if isinstance(text, str):  # Process only if the text is a string
        cleaned_text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # Replace multiple spaces with one
        cleaned_data = cleaned_text.strip()  # Strip leading/trailing spaces
        cleaned.append(cleaned_data)  # Add cleaned text to the list
    else:
        cleaned.append('')  # Add empty string for NaN or non-string values

cleaned

tokens =[word_tokenize(x) for x in cleaned]

stop = set(stopwords.words('english'))  # Fetching the list of English stopwords
stpktn = []  # List to store stopword-removed tokens
for k in range(len(df['selected_text'])):  # Loop through the tokenized text
    p = [i for i in tokens[k] if i not in stop]  # Filter out tokens that are in the stopword list
    stpktn.append(p)

from nltk import pos_tag
nltk.download('averaged_perceptron_tagger_eng')

pos_token=[pos_tag(message) for message in stpktn]

pos_token

from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')

lm=WordNetLemmatizer()

# Function to convert nltk's POS tags to WordNet's POS tags
from nltk.corpus import wordnet
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ  # Adjective
    elif tag.startswith('V'):
        return wordnet.VERB  # Verb
    elif tag.startswith('N'):
        return wordnet.NOUN  # Noun
    elif tag.startswith('R'):
        return wordnet.ADV  # Adverb
    else:
        return wordnet.NOUN  # Default to noun if

lemed_data=[]
for x in range(len(pos_token)):
    lem=[lm.lemmatize(pos_token[x][y][0],get_wordnet_pos(pos_token[x][y][1])) for y in range(len(pos_token[x]))]
    lemed_data.append(lem)

from  sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer()

stem_vec=[' '.join(message) for message in stpktn]

stem_vec

x_vec =cv.fit_transform(stem_vec).toarray()

from sklearn.naive_bayes import MultinomialNB

y=df['sentiment']

mb=MultinomialNB()

mb.fit(x_vec,y)

x_vec[0]

mb.predict([x_vec[0]])

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x_vec, y, test_size=0.1, random_state=42)

model = MultinomialNB()
model.fit(x_train, y_train)

# 4. Make predictions
y_pred = model.predict(x_test)

model.score(x_test,y_test)

from sklearn.linear_model import LogisticRegression

lr_model = LogisticRegression()
lr_model.fit(x_train, y_train)

lr_model.predict(x_test)

lr_model.score(x_test,y_test)

